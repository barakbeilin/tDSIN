{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from enum import Enum, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def bitcost(self, q, target_symbols, is_training, pad_value=0):\n",
    "        \"\"\"\n",
    "        Pads q, creates PC network, calculates cross entropy between output of PC network and target_symbols\n",
    "        :param q: NCHW\n",
    "        :param target_symbols:\n",
    "        :param is_training:\n",
    "        :return: bitcost per symbol: NCHW\n",
    "        \"\"\"\n",
    "        tf_helpers.assert_ndims(q, 4)\n",
    "\n",
    "        with self._building_ctx(self.reuse):\n",
    "            if self.first_mask is None:\n",
    "                self.first_mask = self.create_first_mask()  # DHWio\n",
    "                self.other_mask = self.create_other_mask()  # DHWio\n",
    "\n",
    "            self.reuse = True\n",
    "\n",
    "            targets_one_hot = tf.one_hot(target_symbols, depth=self.L, axis=-1, name='target_symbols')\n",
    "\n",
    "            q_pad = pad_for_probclass3d(\n",
    "                    q, context_size=self.get_context_size(self.config),\n",
    "                    pad_value=pad_value, learn_pad_var=False)\n",
    "            with tf.variable_scope('logits'):\n",
    "                # make it into NCHWT, where T is the channel dim of the conv3d\n",
    "                q_pad = tf.expand_dims(q_pad, -1, name='NCHWT')\n",
    "                logits = self._logits(q_pad, is_training)\n",
    "\n",
    "            if self.config.regularization_factor is not None:\n",
    "                print('Creating PC regularization...')\n",
    "                weights = _get_all_conv3d_weights_in_scope(self._PROBCLASS_SCOPE)\n",
    "                assert len(weights) > 0\n",
    "                reg = self.config.regularization_factor * tf.add_n(list(map(tf.nn.l2_loss, weights)))\n",
    "                tf.losses.add_loss(reg, tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "\n",
    "            if targets_one_hot.shape.is_fully_defined() and logits.shape.is_fully_defined():\n",
    "                tf_helpers.assert_equal_shape(targets_one_hot, logits)\n",
    "\n",
    "            with tf.name_scope('bitcost'):\n",
    "                # softmax_cross_entropy_with_logits is basis e, change base to 2\n",
    "                log_base_change_factor = tf.constant(np.log2(np.e), dtype=tf.float32)\n",
    "                bc = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=logits, labels=targets_one_hot) * log_base_change_factor  # NCHW\n",
    "\n",
    "            return bc # bit cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv3d(x,  # N-CHW-D\n",
    "#            num_outputs,\n",
    "#            filter_shape,  # (C, H, W)\n",
    "#            ):\n",
    "   \n",
    "#     assert len(filter_shape) == 3\n",
    "    \n",
    "    \n",
    "#     strides = [1, 1, 1, 1, 1]\n",
    "#     CONV0_OUT_CH  = 24 \n",
    "    \n",
    "#     # stride 1 by default \n",
    "#     # padding = 0 by deafault\n",
    "    \n",
    "    \n",
    "#     num_inputs = x.shape.as_list()[-1]\n",
    "#     filter_shape = tuple(filter_shape) + (num_inputs, num_outputs)\n",
    "\n",
    "# #     conv = nn.Conv2d(nb_channels, 1, 3, bias=False)\n",
    "# #     with torch.no_grad():\n",
    "# #         conv.weight = nn.Parameter(weights)\n",
    "\n",
    "#     weights = weights * filter_mask\n",
    "#     out = tf.nn.conv3d(x, weights, strides, padding, name='conv3d')\n",
    "#     out = tf.nn.bias_add(out, biases, name='bias3d')\n",
    "#     out = activation_fn(out)\n",
    "#         return out\n",
    "class MaskedConv3d(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels,kernel_size, filter_mask):\n",
    "        \"\"\"\n",
    "        custom config of conv3d:\n",
    "        - use VALID padding (i.e no padding)\n",
    "        - weight initz - xaviel_init\n",
    "        - bias init - zero_init\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.filter_mask = filter_mask\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels ,\n",
    "                        kernel_size=kernel_size, bias= True)\n",
    "        \n",
    "        # initalize layer\n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        nn.init.constant_(t.bias, 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        self._mask_conv_filter()\n",
    "        return self.conv(x)\n",
    "        \n",
    "    def _mask_conv_filter(self):\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight = self.conv.weight * self.filter_mask\n",
    "   \n",
    "    @staticmethod\n",
    "    def create_maskA(kernel_size, filter_shape):\n",
    "            \"\"\"create 5d mask that includes all pixel's in strictly before\"\"\"\n",
    "            K = kernel_size\n",
    "            # mask is DHW\n",
    "            mask = torch.ones(filter_shape, dtype=torch.float32, requires_grad=False)\n",
    "            # zero out D=1,\n",
    "            # - everything to the right of the central pixel, including the central pixel\n",
    "            mask[-1, K // 2, K // 2:] = 0\n",
    "            # - all rows below the central row\n",
    "            mask[-1, K // 2 + 1:, :] = 0\n",
    "\n",
    "            mask.unsqueeze_(-1).unsqueeze_(-1)  # Make into DHWio, for broadcasting with 3D filters\n",
    "            return nn.Parameter(mask)\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_maskB(kernel_size, filter_shape):\n",
    "        \"\"\"create 5d mask that includes all pixel's in before and current pixel\"\"\"\n",
    "        mask = MaskedConv3d.create_first_mask(kernel_size,filter_shape)\n",
    "        K = kernel_size\n",
    "        mask[-1, K // 2, K // 2 + 1] = 1\n",
    "        return mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedResblock(nn.Module):\n",
    "    def __init__(self,channels,filter_shape, kernel_size =3 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        conv0 = self._create_mask_b_conv(channels,filter_shape, kernel_size)\n",
    "        conv2 = self._create_mask_b_conv(channels,filter_shape, kernel_size)\n",
    "        \n",
    "        self.model = nn.Sequential(conv0,nn.ReLU(),conv2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x) + x\n",
    "        \n",
    "    @staticmethod\n",
    "    def _create_mask_b_conv(channels,filter_shape, kernel_size ):\n",
    "        mask = MaskedConv3d.create_other_mask(kernel_size, filter_shape)\n",
    "        return MaskedConv3d(in_channels=channels,out_channels= channels,\n",
    "                         filter_mask=mask)\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbClassifier(nn.Module):\n",
    "    def __init__(self,classifier_in_channels,classifier_out_channels,kernel_size = 3 ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "         \n",
    "        K = self.kernel_size\n",
    "        self.filter_shape = (K // 2 + 1, K, K) # CHW\n",
    "        CONV0_OUT_CH  = 24 \n",
    "        \n",
    "        \n",
    "        mask_A = MaskedConv3d.create_first_mask(self.kernel_size, self.filter_shape)\n",
    "        conv0 = MaskedConv3d(in_channels=classifier_in_channels, out_channels=CONV0_OUT_CH,\n",
    "                             filter_mask=mask_A)\n",
    "        \n",
    "        resblock = MaskedResblock(channels=CONV0_OUT_CH,filter_shape= self.filter_shape,kernel_size= K)\n",
    "        \n",
    "        mask_B = MaskedConv3d.create_other_mask(self.kernel_size, self.filter_shape)\n",
    "        conv2 = MaskedConv3d(in_channels=CONV0_OUT_CH,out_channels= classifier_out_channels,\n",
    "                         filter_mask=mask_B)\n",
    "        \n",
    "        self.model= nn.Sequential(conv0,nn.ReLU(),resblock,conv2,nn.ReLU())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_for_probclass3d(x, context_size, pad_value=0, learn_pad_var=False):\n",
    "        #     \"\"\"\n",
    "        #     :param x: NCHW tensor \n",
    "        #     \"\"\"\n",
    "        #         pad = context_size // 2\n",
    "        #         assert pad >= 1\n",
    "        #         if learn_pad_var:\n",
    "        #             if not isinstance(pad_value, tf.Variable):\n",
    "        #                 print('Warn: Expected tf.Variable for padding, got {}'.format(pad_value))\n",
    "        #             return pc_pad_grad(x, pad, pad_value)\n",
    "\n",
    "        #         pads = [[0, 0],  # don't pad batch dimension\n",
    "        #                 [pad, 0],  # don't pad depth_future, it's not seen by any filter\n",
    "        #                 [pad, pad],\n",
    "        #                 [pad, pad]]\n",
    "        #         assert len(pads) == _get_ndims(x), '{} != {}'.format(len(pads), x.shape)\n",
    "\n",
    "        #         pad_fn = tf.pad if input_is_tf else get_np_pad_fn()\n",
    "        #         return pad_fn(x, pads, constant_values=pad_value)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0162,  0.0916, -0.0714], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = nn.Conv3d(in_channels=3, out_channels=3 ,kernel_size=3, bias = True)\n",
    "print(t.bias )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('torch': conda)",
   "language": "python",
   "name": "python38264bittorchconda5a49bb01e90b47d3b7ca1c7fc4dc1607"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
