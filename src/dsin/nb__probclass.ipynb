{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from enum import Enum, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def bitcost(self, q, target_symbols, is_training, pad_value=0):\n",
    "        \"\"\"\n",
    "        Pads q, creates PC network, calculates cross entropy between output of PC network and target_symbols\n",
    "        :param q: NCHW\n",
    "        :param target_symbols:\n",
    "        :param is_training:\n",
    "        :return: bitcost per symbol: NCHW\n",
    "        \"\"\"\n",
    "        tf_helpers.assert_ndims(q, 4)\n",
    "\n",
    "        with self._building_ctx(self.reuse):\n",
    "            if self.first_mask is None:\n",
    "                self.first_mask = self.create_first_mask()  # DHWio\n",
    "                self.other_mask = self.create_other_mask()  # DHWio\n",
    "\n",
    "            self.reuse = True\n",
    "\n",
    "            targets_one_hot = tf.one_hot(target_symbols, depth=self.L, axis=-1, name='target_symbols')\n",
    "\n",
    "            q_pad = pad_for_probclass3d(\n",
    "                    q, context_size=self.get_context_size(self.config),\n",
    "                    pad_value=pad_value, learn_pad_var=False)\n",
    "            with tf.variable_scope('logits'):\n",
    "                # make it into NCHWT, where T is the channel dim of the conv3d\n",
    "                q_pad = tf.expand_dims(q_pad, -1, name='NCHWT')\n",
    "                logits = self._logits(q_pad, is_training)\n",
    "\n",
    "            if self.config.regularization_factor is not None:\n",
    "                print('Creating PC regularization...')\n",
    "                weights = _get_all_conv3d_weights_in_scope(self._PROBCLASS_SCOPE)\n",
    "                assert len(weights) > 0\n",
    "                reg = self.config.regularization_factor * tf.add_n(list(map(tf.nn.l2_loss, weights)))\n",
    "                tf.losses.add_loss(reg, tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "\n",
    "            if targets_one_hot.shape.is_fully_defined() and logits.shape.is_fully_defined():\n",
    "                tf_helpers.assert_equal_shape(targets_one_hot, logits)\n",
    "\n",
    "            with tf.name_scope('bitcost'):\n",
    "                # softmax_cross_entropy_with_logits is basis e, change base to 2\n",
    "                log_base_change_factor = tf.constant(np.log2(np.e), dtype=tf.float32)\n",
    "                bc = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=logits, labels=targets_one_hot) * log_base_change_factor  # NCHW\n",
    "\n",
    "            return bc # bit cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv3d(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels,kernel_size, filter_mask):\n",
    "        \"\"\"\n",
    "        custom config of conv3d:\n",
    "        - use VALID padding (i.e no padding)\n",
    "        - weight initz - xaviel_init\n",
    "        - bias init - zero_init\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.filter_mask = nn.Parameter(filter_mask)\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels ,\n",
    "                        kernel_size=kernel_size, bias= True)\n",
    "        \n",
    "        # initalize layer\n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        nn.init.constant_(self.conv.bias, 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        self._mask_conv_filter()\n",
    "        return self.conv(x)\n",
    "        \n",
    "    def _mask_conv_filter(self):\n",
    "        with torch.no_grad():\n",
    "            self.conv.weight = self.conv.weight * self.filter_mask\n",
    "   \n",
    "    @staticmethod\n",
    "    def create_maskA(kernel_size, filter_shape):\n",
    "            \"\"\"create 5d mask that includes all pixel's in strictly before\"\"\"\n",
    "            K = kernel_size\n",
    "            # mask is DHW\n",
    "            mask = torch.ones(filter_shape, dtype=torch.float32, requires_grad=False)\n",
    "            # zero out D=1,\n",
    "            # - everything to the right of the central pixel, including the central pixel\n",
    "            mask[-1, K // 2, K // 2:] = 0\n",
    "            # - all rows below the central row\n",
    "            mask[-1, K // 2 + 1:, :] = 0\n",
    "\n",
    "            mask.unsqueeze_(-1).unsqueeze_(-1)  # Make into DHWio, for broadcasting with 3D filters\n",
    "            return mask\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_maskB(kernel_size, filter_shape):\n",
    "        \"\"\"create 5d mask that includes all pixel's in before and current pixel\"\"\"\n",
    "        mask = MaskedConv3d.create_maskA(kernel_size,filter_shape)\n",
    "        K = kernel_size\n",
    "        mask[-1, K // 2, K // 2 + 1] = 1\n",
    "        return mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedResblock(nn.Module):\n",
    "    def __init__(self,channels,filter_shape, kernel_size =3 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        conv0 = self._create_mask_b_conv(channels,filter_shape, kernel_size)\n",
    "        conv2 = self._create_mask_b_conv(channels,filter_shape, kernel_size)\n",
    "        \n",
    "        self.model = nn.Sequential(conv0,nn.ReLU(),conv2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x) + x\n",
    "        \n",
    "    @staticmethod\n",
    "    def _create_mask_b_conv(channels,filter_shape, kernel_size ):\n",
    "        mask = MaskedConv3d.create_maskB(kernel_size, filter_shape)\n",
    "        return MaskedConv3d(in_channels=channels,out_channels= channels,kernel_size=kernel_size,\n",
    "                         filter_mask=mask)\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbClassifier(nn.Module):\n",
    "    def __init__(self,classifier_in_channels,classifier_out_channels,kernel_size = 3 ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "         \n",
    "        K = self.kernel_size\n",
    "        self.filter_shape = (K // 2 + 1, K, K) # CHW\n",
    "        CONV0_OUT_CH  = 24 \n",
    "        \n",
    "        \n",
    "        mask_A = MaskedConv3d.create_maskA(self.kernel_size, self.filter_shape)\n",
    "        conv0 = MaskedConv3d(in_channels=classifier_in_channels, out_channels=CONV0_OUT_CH,\n",
    "                             kernel_size =kernel_size,\n",
    "                             filter_mask=mask_A)\n",
    "        \n",
    "        resblock = MaskedResblock(channels=CONV0_OUT_CH,filter_shape= self.filter_shape,kernel_size= K)\n",
    "        \n",
    "        mask_B = MaskedConv3d.create_maskB(self.kernel_size, self.filter_shape)\n",
    "        conv2 = MaskedConv3d(in_channels=CONV0_OUT_CH,out_channels= classifier_out_channels,\n",
    "                         kernel_size =kernel_size,\n",
    "                         filter_mask=mask_B)\n",
    "        \n",
    "        self.model= nn.Sequential(conv0,nn.ReLU(),resblock,conv2,nn.ReLU())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_for_probclass3d(x, context_size, pad_value=0, learn_pad_var=False):\n",
    "        #     \"\"\"\n",
    "        #     :param x: NCHW tensor \n",
    "        #     \"\"\"\n",
    "        #         pad = context_size // 2\n",
    "        #         assert pad >= 1\n",
    "        #         if learn_pad_var:\n",
    "        #             if not isinstance(pad_value, tf.Variable):\n",
    "        #                 print('Warn: Expected tf.Variable for padding, got {}'.format(pad_value))\n",
    "        #             return pc_pad_grad(x, pad, pad_value)\n",
    "\n",
    "        #         pads = [[0, 0],  # don't pad batch dimension\n",
    "        #                 [pad, 0],  # don't pad depth_future, it's not seen by any filter\n",
    "        #                 [pad, pad],\n",
    "        #                 [pad, pad]]\n",
    "        #         assert len(pads) == _get_ndims(x), '{} != {}'.format(len(pads), x.shape)\n",
    "\n",
    "        #         pad_fn = tf.pad if input_is_tf else get_np_pad_fn()\n",
    "        #         return pad_fn(x, pads, constant_values=pad_value)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOF_CENTERS = 6 \n",
    "pc = ProbClassifier(classifier_in_channels=32, classifier_out_channels=NOF_CENTERS ,kernel_size=3)\n",
    "x = torch.rand([2,32,5,5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Conv2d(3,3,3,bias=True)\n",
    "x.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('torch': conda)",
   "language": "python",
   "name": "python38264bittorchconda5a49bb01e90b47d3b7ca1c7fc4dc1607"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
