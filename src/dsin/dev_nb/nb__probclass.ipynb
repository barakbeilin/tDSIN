{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from enum import Enum, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#  def bitcost(self, q, target_symbols, is_training, pad_value=0):\n",
    "#         \"\"\"\n",
    "#         Pads q, creates PC network, calculates cross entropy between output of PC network and target_symbols\n",
    "#         :param q: NCHW\n",
    "#         :param target_symbols:\n",
    "#         :param is_training:\n",
    "#         :return: bitcost per symbol: NCHW\n",
    "#         \"\"\"\n",
    "#         tf_helpers.assert_ndims(q, 4)\n",
    "\n",
    "#         with self._building_ctx(self.reuse):\n",
    "#             if self.first_mask is None:\n",
    "#                 self.first_mask = self.create_first_mask()  # DHWio\n",
    "#                 self.other_mask = self.create_other_mask()  # DHWio\n",
    "\n",
    "#             self.reuse = True\n",
    "\n",
    "#             targets_one_hot = tf.one_hot(target_symbols, depth=self.L, axis=-1, name='target_symbols')\n",
    "\n",
    "#             q_pad = pad_for_probclass3d(\n",
    "#                     q, context_size=self.get_context_size(self.config),\n",
    "#                     pad_value=pad_value, learn_pad_var=False)\n",
    "#             with tf.variable_scope('logits'):\n",
    "#                 # make it into NCHWT, where T is the channel dim of the conv3d\n",
    "#                 q_pad = tf.expand_dims(q_pad, -1, name='NCHWT')\n",
    "#                 logits = self._logits(q_pad, is_training)\n",
    "\n",
    "#             if self.config.regularization_factor is not None:\n",
    "#                 print('Creating PC regularization...')\n",
    "#                 weights = _get_all_conv3d_weights_in_scope(self._PROBCLASS_SCOPE)\n",
    "#                 assert len(weights) > 0\n",
    "#                 reg = self.config.regularization_factor * tf.add_n(list(map(tf.nn.l2_loss, weights)))\n",
    "#                 tf.losses.add_loss(reg, tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "\n",
    "#             if targets_one_hot.shape.is_fully_defined() and logits.shape.is_fully_defined():\n",
    "#                 tf_helpers.assert_equal_shape(targets_one_hot, logits)\n",
    "\n",
    "#             with tf.name_scope('bitcost'):\n",
    "#                 # softmax_cross_entropy_with_logits is basis e, change base to 2\n",
    "#                 log_base_change_factor = tf.constant(np.log2(np.e), dtype=tf.float32)\n",
    "#                 bc = tf.nn.softmax_cross_entropy_with_logits(\n",
    "#                     logits=logits, labels=targets_one_hot) * log_base_change_factor  # NCHW\n",
    "\n",
    "#             return bc # bit cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class MaskedConv3d(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels,kernel_size, filter_mask):\n",
    "        \"\"\"\n",
    "        custom config of conv3d:\n",
    "        - use VALID padding (i.e no padding)\n",
    "        - weight initz - xaviel_init\n",
    "        - bias init - zero_init\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.filter_mask = nn.Parameter(filter_mask)\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels, out_channels=out_channels ,\n",
    "                        kernel_size=kernel_size, bias= True)\n",
    "        \n",
    "        # initalize layer\n",
    "        nn.init.xavier_uniform_(self.conv.weight)\n",
    "        nn.init.constant_(self.conv.bias, 0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        self._mask_conv_filter()\n",
    "        return self.conv(x)\n",
    "        \n",
    "    def _mask_conv_filter(self):\n",
    "        with torch.no_grad():\n",
    "            print((self.conv.weight.shape))\n",
    "            print((self.filter_mask.shape))\n",
    "            self.conv.weight = self.conv.weight * self.filter_mask\n",
    "   \n",
    "    @staticmethod\n",
    "    def create_maskA(kernel_size, filter_shape):\n",
    "            \"\"\"create 5d mask that includes all pixel's in strictly before\"\"\"\n",
    "            K = kernel_size\n",
    "            # mask is DHW\n",
    "            mask = torch.ones(filter_shape, dtype=torch.float32, requires_grad=False)\n",
    "            # zero out D=1,\n",
    "            # - everything to the right of the central pixel, including the central pixel\n",
    "            mask[-1, K // 2, K // 2:] = 0\n",
    "            # - all rows below the central row\n",
    "            mask[-1, K // 2 + 1:, :] = 0\n",
    "\n",
    "            mask.unsqueeze_(-1).unsqueeze_(-1)  # Make into DHWio, for broadcasting with 3D filters\n",
    "            return mask\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_maskB(kernel_size, filter_shape):\n",
    "        \"\"\"create 5d mask that includes all pixel's in before and current pixel\"\"\"\n",
    "        K = kernel_size\n",
    "        # mask is DHW\n",
    "        mask = torch.ones(filter_shape, dtype=torch.float32, requires_grad=False)\n",
    "        # zero out D=1,\n",
    "        # - everything to the right of the central pixel, including the central pixel\n",
    "        mask[-1, K // 2, K // 2 + 1:] = 0\n",
    "        # - all rows below the central row\n",
    "        mask[-1, K // 2 + 1:, :] = 0\n",
    "\n",
    "        mask.unsqueeze_(-1).unsqueeze_(-1)  # Make into DHWio, for broadcasting with 3D filters\n",
    "        return mask\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class MaskedResblock(nn.Module):\n",
    "    def __init__(self,channels,filter_shape, kernel_size ):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        conv0 = self._create_mask_b_conv(channels,filter_shape, kernel_size)\n",
    "        conv2 = self._create_mask_b_conv(channels,filter_shape, kernel_size)\n",
    "        \n",
    "        self.model = nn.Sequential(conv0,nn.ReLU(),conv2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x) + x[..., 2:, 2:-2, 2:-2, :] # fit the padding\n",
    "        \n",
    "    @staticmethod\n",
    "    def _create_mask_b_conv(channels,filter_shape, kernel_size ):\n",
    "        mask = MaskedConv3d.create_maskB(kernel_size, filter_shape)\n",
    "        return MaskedConv3d(in_channels=channels,out_channels= channels,kernel_size=kernel_size,\n",
    "                         filter_mask=mask)\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "class ProbClassifier(nn.Module):\n",
    "    def __init__(self,classifier_in_channels,classifier_out_channels,kernel_size = 3 ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "                \n",
    "        \n",
    "                \n",
    "        K = self.kernel_size\n",
    "        self.filter_shape = (K // 2 + 1, K, K) # CHW\n",
    "        CONV0_OUT_CH  = 24 \n",
    "        \n",
    "        \n",
    "        mask_A = MaskedConv3d.create_maskA(self.kernel_size, self.filter_shape)\n",
    "        conv0 = MaskedConv3d(in_channels=classifier_in_channels, out_channels=CONV0_OUT_CH,\n",
    "                             kernel_size =kernel_size,\n",
    "                             filter_mask=mask_A)\n",
    "        \n",
    "        resblock = MaskedResblock(channels=CONV0_OUT_CH,filter_shape= self.filter_shape,kernel_size= K)\n",
    "        \n",
    "        mask_B = MaskedConv3d.create_maskB(self.kernel_size, self.filter_shape)\n",
    "        conv2 = MaskedConv3d(in_channels=CONV0_OUT_CH,out_channels= classifier_out_channels,\n",
    "                         kernel_size =kernel_size,\n",
    "                         filter_mask=mask_B)\n",
    "        \n",
    "        self.model= nn.Sequential(self.zero_pad_layer(),conv0,nn.ReLU(),resblock,conv2,nn.ReLU())\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "    def zero_pad_layer(self):\n",
    "        \"\"\"\n",
    "        :param x: NCHW tensorflow Tensor or numpy array\n",
    "        \"\"\"\n",
    "        nof_conv_layers_classifier = 4 # 4 convd3d layers\n",
    "        context_size = nof_conv_layers_classifier *(self.kernel_size - 1) + 1\n",
    "        pad = context_size // 2 # 4\n",
    "        \n",
    "        #padding_left , padding_right , padding_top , padding_bottom , padding_front , padding_back\n",
    "\n",
    "        pads= (pad,pad,pad,pad,pad,0) \n",
    "        \n",
    "        return nn.ConstantPad3d(pads,value =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOF_CENTERS = 6 \n",
    "# pc = ProbClassifier(classifier_in_channels=32, classifier_out_channels=NOF_CENTERS ,kernel_size=3)\n",
    "# x = torch.rand([1,2,32,5,5])\n",
    "# # nt(pc)\n",
    "# pc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 3\n",
    "# mask = MaskedConv3d.create_maskB(kernel_size= 3, filter_shape=(K // 2 + 1, K, K))\n",
    "# print(mask.shape)\n",
    "\n",
    "# conv0 = MaskedConv3d(in_channels = 7, out_channels=5,kernel_size =3, filter_mask=mask)\n",
    "# print(conv0.conv.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad = 1\n",
    "# pads= (pad,pad,pad,pad,pad,0) \n",
    "# l = nn.ConstantPad3d(pads,value =2)\n",
    "# x = torch.ones([2,2,2,2])\n",
    "# print(x.shape)\n",
    "# print(x)\n",
    "# print(l(x).shape)\n",
    "# print(l(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted nb__probclass.ipynb to exp/probclass.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py nb__probclass.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('torch': conda)",
   "language": "python",
   "name": "python38264bittorchconda5a49bb01e90b47d3b7ca1c7fc4dc1607"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
