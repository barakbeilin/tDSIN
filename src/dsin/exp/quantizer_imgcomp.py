
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/nb__quantizer_imgcomp.ipynb
import torch
import torch.nn.functional as F
from torch import nn
from enum import Enum, auto


"""Quantizer layer."""
class ChannelOrder(Enum):
    NHWC = auto()
    NCHW = auto()

class Quantizer(nn.Module):
    def __init__(self, device,
                 num_centers,
                 centers_initial_range,
                 regularization_factor_centers,
                 sigma,
                 reg_dtype = torch.float32):
        super().__init__()
        self.cdevice = device
        self.num_centers = (num_centers,)
        self.centers_initial_range = centers_initial_range
        self.sigma = sigma
        self.reg = \
        torch.torch.as_tensor(regularization_factor_centers, dtype=reg_dtype)

        self._create_centers_variable()

    def _create_centers_variable(self, dtype= torch.float32):  # (C, L) or (L,)
        assert self.num_centers is not None
        minval, maxval = map(int, self.centers_initial_range)
        # create a tensor of size with values drawn
        # from uniform distribution
        centers = torch.rand(*self.num_centers, requires_grad=False, dtype=dtype, device = self.cdevice) \
            * (maxval-minval) + minval
        # Wrapping with nn.Parameter ensures it is copied to gpu when .to('cuda') is called
        self.centers = nn.Parameter(centers)

    def _create_centers_regularization_term(self):
        if self.regularization_factor_centers != 0:
            # calculate half the l2 norm  like tf.nn.l2_loss(centers)
            cetners_reg = 0.5 * reg * torch.nn.norm(self.centers)

    def __repr__(self):
        return f'{self.__class__.__name__}(sigma={self.sigma})'


    def forward(self, x, dataformat: ChannelOrder):
        assert x.dtype == torch.float32, 'x should be float32'
        assert self.centers.dtype == torch.float32, 'centers should be float32'
        assert len(x) == 4, f'x should be NCHW or NHWC got {len(x)} instead'
        assert len(self.centers) == 1, f'centers should be (L,), got {len(centers)}'

        # improve numerics by calculating using NCHW
        if dataformat==ChannelOrder.NHWC:
            x = self.__permute_NHWC_to_NCHW(x)

        x_soft, x_hard, symbols_hard= self._quantize(x)

        #return tensors in the original channel order
        if dataformat==ChannelOrder.NHWC:
            return tuple(map(self.permute_NCHW_to_NHWC,
                             (x_soft, x_hard, symbols_hard)))
        else:
            return x_soft, x_hard, symbols_hard


    def _quantize(self, x, sigma):

        N, C, H, W = x.shape

        # Turn each image into vector, i.e. make x into NCm1, where m=H*W
        x = x.view(N, C, H*W, 1)
        # shape- NCmL, calc distance to l-th center
        d = torch.pow(x - self.centers, 2)
        # shape- NCmL, \sum_l d[..., l] sums to 1
        phi_soft = F.softmax(-self.sigma * d, dim=-1)
        # - Calcualte soft assignements ---
        # NCm, soft assign x to levels
        x_soft = torch.sum(self.levels * phi_soft, dim=-1)
        # NCHW
        x_soft = x_soft.view(N, C, H, W)

        ######################
        # Calcualte hard assignements for the forward pass, keep gards for the backward
        ######################

        # NCm, symbols_hard[..., i] contains index of symbol closest to each pixel
        # detach d to use values without affecting the gradients
        _, symbols_hard = torch.min(d.detach(), dim=-1)
        # NCHW
        symbols_hard = symbols_hard.view(N, C, H, W)
        # NCHW, contains value of symbol to use
        x_hard = self.levels[symbols_hard]

        x_soft.data = x_hard  # assign data, keep gradient
        return x_soft, x_hard, symbols_hard

    @staticmethod
    def permute_NHWC_to_NCHW(t):
        N, H, W, C = 0 ,1 ,2 ,3
        return t.permute(N, C, H, W)

    @staticmethod
    def permute_NCHW_to_NHWC(t):
        N, C, H, W = 0 ,1 ,2 ,3
        return t.permute(N, H, W, C)
